{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_file = 'comments_2022.csv'\n",
    "df = pd.read_csv(dataset_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc47e4a",
   "metadata": {},
   "source": [
    "# Dataset clean step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d05cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort subreddits by number of comments\n",
    "sub_coms = df.groupby('subreddit')['id'].nunique().to_frame()\n",
    "sub_coms.columns = ['count']\n",
    "sub_coms.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort subreddits by number of authors\n",
    "sub_auth = df.groupby('subreddit')['author'].nunique().to_frame()\n",
    "sub_auth.columns = ['count']\n",
    "sub_auth.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too much subreddits\n",
    "print(f'Number of subreddits original: {len(sub_coms)}')\n",
    "# Remove all subreddits that has less than 2 comments\n",
    "df = df.loc[~df['subreddit'].isin(list(sub_coms[sub_coms['count']<2].index))]\n",
    "print(f'All subreddits with more than 1 comment: {len(df.subreddit.unique())}')\n",
    "# Remove all subreddits that has less than 2 authors\n",
    "df = df.loc[~df['subreddit'].isin(list(sub_auth[sub_auth['count']<2].index))]\n",
    "print(f'All subreddits with more than 2 authors: {len(df.subreddit.unique())}')\n",
    "# Witchcraft has no cultural appropriation themes, maybe conflicting keywords.\n",
    "df = df.loc[df['subreddit'] != 'witchcraft']\n",
    "# Same for KUWTK\n",
    "df = df.loc[df['subreddit'] != 'KUWTK']\n",
    "\n",
    "len(df.subreddit.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all comments that has less than 4 words\n",
    "print(f'Number of comments: {len(df.id.unique())}')\n",
    "df = df[df['body'].str.split().str.len() > 3]\n",
    "print(f'Number of comments after removal of short: {len(df.id.unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09875d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all lines where subreddit or author is deleted\n",
    "print(f'Number of subreddits: {len(df.subreddit.unique())}')\n",
    "df = df.loc[(~df['author'].str.contains('deleted')) | (~df['subreddit'].str.contains('deleted'))]\n",
    "print(f'Number of subreddits after clean 1: {len(df.subreddit.unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd83f2c2",
   "metadata": {},
   "source": [
    "# Dataset enrichment with topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c98c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3 Ada model for topic extraction\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Authenticate to OpenAI API\n",
    "api_key = '<your-api-key-here>''\n",
    "headers = {'Content-Type': 'application/json',\n",
    "           'Authorization': f'Bearer {api_key}'}\n",
    "\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# Define the function to extract topics\n",
    "def extract_topics(text):\n",
    "    #parsed_text = text.replace('\\n',' ')\n",
    "    #parsed_text = re.sub(r'[^\\w]', '', parsed_text)\n",
    "    data = {\n",
    "        \"prompt\":f\"Comment: {text}\\n\\nTopics:\", \n",
    "        \"temperature\":0.5,\n",
    "        \"max_tokens\":60,\n",
    "        \"model\": \"text-ada-001\",\n",
    "        \"stop\": [\"\\n\\n\"]\n",
    "    }\n",
    "    \n",
    "    # Send request\n",
    "    response = session.post('https://api.openai.com/v1/completions', headers=headers, data=json.dumps(data))\n",
    "    \n",
    "    # Extract topics\n",
    "    try:\n",
    "        topics = response.json()['choices'][0]['text'].strip()\n",
    "        topics = topics.split(', ')\n",
    "    except:\n",
    "        topics = []\n",
    "\n",
    "    return [t.lower() for t in topics if not any(i.isdigit() for i in t)]\n",
    "\n",
    "# Apply the function to the 'text' column of the dataframe\n",
    "df['topics'] = df['body'].apply(lambda x: extract_topics(x))\n",
    "#df_test = df.copy()\n",
    "#df_test['topics'] = df['body'].iloc[:10].apply(lambda x: extract_topics(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('comments_2022_step1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9625a52",
   "metadata": {},
   "source": [
    "# Dataset clean step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('comments_2022_step1.csv', header=0)\n",
    "df['topics'] = df['topics'].apply(lambda x: x.replace('[','').replace(']','').replace(\"'\",'').split(', '))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Clean topic names\n",
    "df['topics'] = df['topics'].apply(lambda x: [t.replace('\"', '').replace(\"'\",'') for t in x])\n",
    "# Clean other errors\n",
    "df['topics'] = df['topics'].apply(lambda x: [t.replace('-',' ').replace('r/','') for t in x])\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if '_' not in t])\n",
    "# Remove too short, too long and question topics\n",
    "def check_splits(topic):\n",
    "    to_return = []\n",
    "    a = re.split(r' |,|\\\\n|\\.|;', topic)\n",
    "    # Don't add topics with more than 2 words\n",
    "    if len(a) < 3:\n",
    "        for x in a:\n",
    "            if len(x) > 3:\n",
    "                to_return.append(x)\n",
    "    return ' '.join(to_return)\n",
    "df['topics'] = df['topics'].apply(lambda x: [check_splits(t) for t in x])\n",
    "# Remove all hashtags\n",
    "df['topics'] = df['topics'].apply(lambda x: [t.replace('#','') for t in x])\n",
    "# Remove html chars\n",
    "df['topics'] = df['topics'].apply(lambda x: [t.replace('&gt;','').replace('&lt;','') for t in x])\n",
    "# Remove links\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if \"http\" not in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all topics that are less than 4 letters\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if len(t) > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ee7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all non english topics\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "def is_english(topic):\n",
    "    a = topic.split(' ')\n",
    "    to_return = []\n",
    "    for x in a:\n",
    "        if x.lower() in english_words:\n",
    "            to_return.append(x)\n",
    "    return ' '.join(to_return)\n",
    "\n",
    "df['topics'] = df['topics'].apply(lambda x: [is_english(t) for t in x])\n",
    "# Remove now empty topics\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if len(t)>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e8c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def is_noun(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Part of speech tagging\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "    # Check if at least one word is a noun\n",
    "    for word, tag in tagged_words:\n",
    "        if tag == 'NN':\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if is_noun(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aed76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual clean\n",
    "def is_noun(text):\n",
    "    if len(text.split(\" \")) == 1:\n",
    "        if text.endswith(\"ent\") or text.endswith('er') or text.endswith('est') or text.endswith('ive') or text.endswith('y') or text.endswith(\"ful\") or text.endswith('al') or text.endswith('ung') or text.endswith('th') or text.endswith('ing'):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if is_noun(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82198d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def remove_verbs(df, column):\n",
    "    for i, row in df.iterrows():\n",
    "        strings = row[column]\n",
    "        strings = [string for string in strings if not contains_verb(string)]\n",
    "        # Update the row with the list of strings without verbs\n",
    "        df.at[i, column] = strings\n",
    "    return df\n",
    "\n",
    "def contains_verb(string):\n",
    "    words = nltk.word_tokenize(string)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    # Check if the string contains a verb\n",
    "    return any(tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] for word, tag in tagged_words)\n",
    "\n",
    "df = remove_verbs(df, 'topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70051714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def replace_adj_with_noun(df, column):\n",
    "    for i, row in df.iterrows():\n",
    "        strings = row[column]\n",
    "        new_strings = []\n",
    "        for string in strings:\n",
    "            words = nltk.word_tokenize(string)\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            new_string = []\n",
    "            for word, tag in tagged_words:\n",
    "                # Replace the adjective with its corresponding noun\n",
    "                if tag == 'JJ':\n",
    "                    synsets = wordnet.synsets(word, pos='a')\n",
    "                    if len(synsets) > 0:\n",
    "                        lemmas = synsets[0].lemmas()\n",
    "                        if len(lemmas) > 0:\n",
    "                            p = lemmas[0].pertainyms()\n",
    "                            if len(p) > 0:\n",
    "                                new_word = p[0].name()\n",
    "                                new_string.append(new_word)\n",
    "                                continue\n",
    "                new_string.append(word)\n",
    "            new_strings.append(' '.join(new_string))\n",
    "        # Update the row with the list of strings with replaced adjectives\n",
    "        df.at[i, column] = new_strings\n",
    "    return df\n",
    "\n",
    "df = replace_adj_with_noun(df, 'topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b92573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def uniform_strings(df, column):\n",
    "    # Create a dictionary to store the mapping from words to uniformed strings\n",
    "    mapping = {}\n",
    "    uniformed_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        strings = row[column]\n",
    "        for j, string in enumerate(strings):\n",
    "            # Split the string into words and sort them\n",
    "            words = string.split()\n",
    "            words.sort()\n",
    "            # Join the words back together to create a key for the mapping\n",
    "            key = \" \".join(words)\n",
    "            # If the key is not in the mapping, add it and the original string\n",
    "            if key not in mapping:\n",
    "                mapping[key] = string\n",
    "                uniformed_strings.append(string)\n",
    "            # Replace the string with the uniformed string\n",
    "            strings[j] = mapping[key]\n",
    "        # Update the row with the uniformed strings\n",
    "        df.at[i, column] = strings\n",
    "    return df\n",
    "\n",
    "df = uniform_strings(df, 'topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19975575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "combi = set([t for x in list(df['topics']) for t in x if len(t.split(\" \")) > 1])\n",
    "\n",
    "def check_combi(strings):\n",
    "    new_list = strings\n",
    "    word_combinations = list(combinations(strings, 2))\n",
    "    word_combinations = [t for t in word_combinations if len(t) < 3]\n",
    "    for word in word_combinations:\n",
    "        if ' '.join(word) in combi:\n",
    "            if word[0] in new_list:\n",
    "                new_list.remove(word[0])\n",
    "            if word[1] in new_list:\n",
    "                new_list.remove(word[1])\n",
    "            new_list.append(' '.join(word))\n",
    "    return new_list\n",
    "\n",
    "df['topics'] = df['topics'].apply(check_combi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e54c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all comments without topics\n",
    "print(f'Number of comments: {len(df.id.unique())}')\n",
    "df = df.loc[df['topics'].apply(lambda x: len(x) > 0)]\n",
    "print(f'Number of comments after cleaning: {len(df.id.unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('comments_2022_step2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fe713",
   "metadata": {},
   "source": [
    "# Enrichment with sub-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('comments_2022_step2.csv', header=0)\n",
    "df['topics'] = df['topics'].apply(lambda x: x.replace('[','').replace(']','').replace(\"'\",'').split(', '))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96aec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove AutoModerator\n",
    "df = df.loc[df['author']!='AutoModerator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20beb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_keywords = [    \"symbol\", \"art\", \"flag\", \"history\", \"tattoo\", \"mask\", \"tribe\", \"ancestral\",    \"heritage\",    \"totem\",    \"headdress\",    \"regalia\",    \"jewelry\",    \"indigenous\",    \"ceremonial\",    \"ritual\",    \"sacred\",    \"spiritual\",    \"identity\",    \"legacy\"]\n",
    "\n",
    "m_keywords = [    \"hair\", \"haircut\", \"dread\", \"cloth\", \"cornrow\", \"style\", \"pierc\", \"wore\", \"coloniz\", \"wear\", \"misappropriation\",    \"ethnic\",    \"native\",    \"dress\",    \"attire\",    \"outfit\"]\n",
    "\n",
    "l_keywords = [    \"philosoph\", \"english\", \"hola\", \"language\", \"translate\", \"interpreter\", \"name\", \"linguistic\",    \"vernacular\",    \"dialect\",    \"accent\",    \"identity\",    \"heritage\",    \"expression\",    \"borrowing\"]\n",
    "\n",
    "s_keywords = [    \"blackfac\", \"whitewash\", \"song\", \"sing\", \"movie\", \"caricat\", \"equality\", \"character\", \"racis\",\"gatekeep\", \"born\", \"stereotyping\",    \"misrepresentation\",    \"stereotype\",    \"portrayal\",    \"sensitivity\",    \"awareness\",    \"diversity\",    \"respect\",    \"tolerance\"]\n",
    "\n",
    "i_keywords = [    \"ramen\", \"jew\", \"pagan\", \"pizza\", \"preach\", \"gospel\", \"coffee\", \"book\",\"food\", \"christ\", \"cook\", \"recipe\", \"siddur\", \"pray\", \"myth\", \"bow\", \"religion\", \"insensitivity\",    \"religious\",    \"practices\",    \"respect\",    \"tolerance\",    \"identity\",    \"heritage\",    \"expression\"]\n",
    "\n",
    "e_keywords = [   \"compan\", \"anime\", \"commerc\", \"rap\", \"exploit\", \"sexuali\", \"music\", \"exploitation\",    \"commodification\",    \"profit\",    \"commercialization\",    \"tourism\",    \"revenue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtypes(comment):\n",
    "    sub_types = []\n",
    "    for word in a_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('A')\n",
    "    for word in m_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('M')\n",
    "    for word in l_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('L')\n",
    "    for word in s_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('S')\n",
    "    for word in i_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('I')\n",
    "    for word in e_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('E')\n",
    "    return list(set(sub_types))\n",
    "\n",
    "df['sub-types'] = df['body'].apply(get_subtypes)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9987f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['sub-types'].map(len)==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['subreddit']!='[deleted]']\n",
    "df = df.loc[df['author']!='[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('comments_2022_step3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
