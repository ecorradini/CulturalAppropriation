{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e182106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hqqvfvb</td>\n",
       "      <td>t1_hqqt3r3</td>\n",
       "      <td>BasicComplexities</td>\n",
       "      <td>VaushV</td>\n",
       "      <td>\\n&amp;gt;. You use French people flippantly, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hqqxbu8</td>\n",
       "      <td>t1_hqqhgr2</td>\n",
       "      <td>RagingAardvark</td>\n",
       "      <td>daddit</td>\n",
       "      <td>Thank you! I was actually recently mulling ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hqqz28x</td>\n",
       "      <td>t3_rsooda</td>\n",
       "      <td>malarky-b</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>Maybe I'm just tired but I don't get the comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hqr027z</td>\n",
       "      <td>t1_hqqz64g</td>\n",
       "      <td>AdventurousAnxiety78</td>\n",
       "      <td>Afghan</td>\n",
       "      <td>No, the coins are one of many diverse versions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hqr0j0w</td>\n",
       "      <td>t3_rt7dak</td>\n",
       "      <td>stjeana</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>Cultural appropriation is vs and doesnt have a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   parent_id                author       subreddit  \\\n",
       "0  hqqvfvb  t1_hqqt3r3     BasicComplexities          VaushV   \n",
       "1  hqqxbu8  t1_hqqhgr2        RagingAardvark          daddit   \n",
       "2  hqqz28x   t3_rsooda             malarky-b   asianamerican   \n",
       "3  hqr027z  t1_hqqz64g  AdventurousAnxiety78          Afghan   \n",
       "4  hqr0j0w   t3_rt7dak               stjeana  TooAfraidToAsk   \n",
       "\n",
       "                                                body  \n",
       "0  \\n&gt;. You use French people flippantly, but ...  \n",
       "1  Thank you! I was actually recently mulling ove...  \n",
       "2  Maybe I'm just tired but I don't get the comme...  \n",
       "3  No, the coins are one of many diverse versions...  \n",
       "4  Cultural appropriation is vs and doesnt have a...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset_file = 'comments_2022.csv'\n",
    "df = pd.read_csv(dataset_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc47e4a",
   "metadata": {},
   "source": [
    "# Dataset clean step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b3d05cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>witchcraft</th>\n",
       "      <td>4747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AmItheAsshole</th>\n",
       "      <td>2564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TooAfraidToAsk</th>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AskReddit</th>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unpopularopinion</th>\n",
       "      <td>698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StonerPhilosophy</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stoicism</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steam</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StarWarsCantina</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zenbuddhism</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3027 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count\n",
       "subreddit              \n",
       "witchcraft         4747\n",
       "AmItheAsshole      2564\n",
       "TooAfraidToAsk      914\n",
       "AskReddit           840\n",
       "unpopularopinion    698\n",
       "...                 ...\n",
       "StonerPhilosophy      1\n",
       "Stoicism              1\n",
       "Steam                 1\n",
       "StarWarsCantina       1\n",
       "zenbuddhism           1\n",
       "\n",
       "[3027 rows x 1 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort subreddits by number of comments\n",
    "sub_coms = df.groupby('subreddit')['id'].nunique().to_frame()\n",
    "sub_coms.columns = ['count']\n",
    "sub_coms.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "87de484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AmItheAsshole</th>\n",
       "      <td>2078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TooAfraidToAsk</th>\n",
       "      <td>738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AskReddit</th>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unpopularopinion</th>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facepalm</th>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Squidbillies</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Squamish</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sprechstunde</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpiritAnimals</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zenbuddhism</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3027 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count\n",
       "subreddit              \n",
       "AmItheAsshole      2078\n",
       "TooAfraidToAsk      738\n",
       "AskReddit           715\n",
       "unpopularopinion    536\n",
       "facepalm            382\n",
       "...                 ...\n",
       "Squidbillies          1\n",
       "Squamish              1\n",
       "Sprechstunde          1\n",
       "SpiritAnimals         1\n",
       "zenbuddhism           1\n",
       "\n",
       "[3027 rows x 1 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort subreddits by number of authors\n",
    "sub_auth = df.groupby('subreddit')['author'].nunique().to_frame()\n",
    "sub_auth.columns = ['count']\n",
    "sub_auth.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5184aba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subreddits original: 3027\n",
      "All subreddits with more than 1 comment: 1670\n",
      "All subreddits with more than 2 authors: 1616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1614"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Too much subreddits\n",
    "print(f'Number of subreddits original: {len(sub_coms)}')\n",
    "# Remove all subreddits that has less than 2 comments\n",
    "df = df.loc[~df['subreddit'].isin(list(sub_coms[sub_coms['count']<2].index))]\n",
    "print(f'All subreddits with more than 1 comment: {len(df.subreddit.unique())}')\n",
    "# Remove all subreddits that has less than 2 authors\n",
    "df = df.loc[~df['subreddit'].isin(list(sub_auth[sub_auth['count']<2].index))]\n",
    "print(f'All subreddits with more than 2 authors: {len(df.subreddit.unique())}')\n",
    "# Witchcraft has no cultural appropriation themes, maybe conflicting keywords.\n",
    "df = df.loc[df['subreddit'] != 'witchcraft']\n",
    "# Same for KUWTK\n",
    "df = df.loc[df['subreddit'] != 'KUWTK']\n",
    "\n",
    "len(df.subreddit.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "999b1404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments: 24945\n",
      "Number of comments after removal of short: 24422\n"
     ]
    }
   ],
   "source": [
    "# Remove all comments that has less than 4 words\n",
    "print(f'Number of comments: {len(df.id.unique())}')\n",
    "df = df[df['body'].str.split().str.len() > 3]\n",
    "print(f'Number of comments after removal of short: {len(df.id.unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e09875d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subreddits: 1613\n",
      "Number of subreddits after clean 1: 1613\n"
     ]
    }
   ],
   "source": [
    "# Remove all lines where subreddit or author is deleted\n",
    "print(f'Number of subreddits: {len(df.subreddit.unique())}')\n",
    "df = df.loc[(~df['author'].str.contains('deleted')) | (~df['subreddit'].str.contains('deleted'))]\n",
    "print(f'Number of subreddits after clean 1: {len(df.subreddit.unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd83f2c2",
   "metadata": {},
   "source": [
    "# Dataset enrichment with topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d0c98c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3 Ada model for topic extraction\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Authenticate to OpenAI API\n",
    "api_key = '<your-api-key-here>''\n",
    "headers = {'Content-Type': 'application/json',\n",
    "           'Authorization': f'Bearer {api_key}'}\n",
    "\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# Define the function to extract topics\n",
    "def extract_topics(text):\n",
    "    #parsed_text = text.replace('\\n',' ')\n",
    "    #parsed_text = re.sub(r'[^\\w]', '', parsed_text)\n",
    "    data = {\n",
    "        \"prompt\":f\"Comment: {text}\\n\\nTopics:\", \n",
    "        \"temperature\":0.5,\n",
    "        \"max_tokens\":60,\n",
    "        \"model\": \"text-ada-001\",\n",
    "        \"stop\": [\"\\n\\n\"]\n",
    "    }\n",
    "    \n",
    "    # Send request\n",
    "    response = session.post('https://api.openai.com/v1/completions', headers=headers, data=json.dumps(data))\n",
    "    \n",
    "    # Extract topics\n",
    "    try:\n",
    "        topics = response.json()['choices'][0]['text'].strip()\n",
    "        topics = topics.split(', ')\n",
    "    except:\n",
    "        topics = []\n",
    "\n",
    "    return [t.lower() for t in topics if not any(i.isdigit() for i in t)]\n",
    "\n",
    "# Apply the function to the 'text' column of the dataframe\n",
    "df['topics'] = df['body'].apply(lambda x: extract_topics(x))\n",
    "#df_test = df.copy()\n",
    "#df_test['topics'] = df['body'].iloc[:10].apply(lambda x: extract_topics(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "87fe362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('comments_2022_step1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9625a52",
   "metadata": {},
   "source": [
    "# Dataset clean step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d159c253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hqqvfvb</td>\n",
       "      <td>t1_hqqt3r3</td>\n",
       "      <td>BasicComplexities</td>\n",
       "      <td>VaushV</td>\n",
       "      <td>\\n&amp;gt;. You use French people flippantly, but ...</td>\n",
       "      <td>[racist slurs, non-racial slurs, words, slurs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hqqxbu8</td>\n",
       "      <td>t1_hqqhgr2</td>\n",
       "      <td>RagingAardvark</td>\n",
       "      <td>daddit</td>\n",
       "      <td>Thank you! I was actually recently mulling ove...</td>\n",
       "      <td>[encanto, diversity, cultural appropriation, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hqqz28x</td>\n",
       "      <td>t3_rsooda</td>\n",
       "      <td>malarky-b</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>Maybe I'm just tired but I don't get the comme...</td>\n",
       "      <td>[ad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hqr027z</td>\n",
       "      <td>t1_hqqz64g</td>\n",
       "      <td>AdventurousAnxiety78</td>\n",
       "      <td>Afghan</td>\n",
       "      <td>No, the coins are one of many diverse versions...</td>\n",
       "      <td>[hazara, traditional dress, turkic people, ara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hqr0j0w</td>\n",
       "      <td>t3_rt7dak</td>\n",
       "      <td>stjeana</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>Cultural appropriation is vs and doesnt have a...</td>\n",
       "      <td>[cultural appropriation, cool, respect, culture]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   parent_id                author       subreddit  \\\n",
       "0  hqqvfvb  t1_hqqt3r3     BasicComplexities          VaushV   \n",
       "1  hqqxbu8  t1_hqqhgr2        RagingAardvark          daddit   \n",
       "2  hqqz28x   t3_rsooda             malarky-b   asianamerican   \n",
       "3  hqr027z  t1_hqqz64g  AdventurousAnxiety78          Afghan   \n",
       "4  hqr0j0w   t3_rt7dak               stjeana  TooAfraidToAsk   \n",
       "\n",
       "                                                body  \\\n",
       "0  \\n&gt;. You use French people flippantly, but ...   \n",
       "1  Thank you! I was actually recently mulling ove...   \n",
       "2  Maybe I'm just tired but I don't get the comme...   \n",
       "3  No, the coins are one of many diverse versions...   \n",
       "4  Cultural appropriation is vs and doesnt have a...   \n",
       "\n",
       "                                              topics  \n",
       "0     [racist slurs, non-racial slurs, words, slurs]  \n",
       "1  [encanto, diversity, cultural appropriation, w...  \n",
       "2                                               [ad]  \n",
       "3  [hazara, traditional dress, turkic people, ara...  \n",
       "4   [cultural appropriation, cool, respect, culture]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('comments_2022_step1.csv', header=0)\n",
    "df['topics'] = df['topics'].apply(lambda x: x.replace('[','').replace(']','').replace(\"'\",'').split(', '))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a064db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Clean topic names\n",
    "df['topics'] = df['topics'].apply(lambda x: [t.replace('\"', '').replace(\"'\",'') for t in x])\n",
    "# Clean other errors\n",
    "df['topics'] = df['topics'].apply(lambda x: [t.replace('-',' ').replace('r/','') for t in x])\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if '_' not in t])\n",
    "# Remove too short, too long and question topics\n",
    "def check_splits(topic):\n",
    "    to_return = []\n",
    "    a = re.split(r' |,|\\\\n|\\.|;', topic)\n",
    "    # Don't add topics with more than 2 words\n",
    "    if len(a) < 3:\n",
    "        for x in a:\n",
    "            if len(x) > 3:\n",
    "                to_return.append(x)\n",
    "    return ' '.join(to_return)\n",
    "df['topics'] = df['topics'].apply(lambda x: [check_splits(t) for t in x])\n",
    "# Remove all hashtags\n",
    "df['topics'] = df['topics'].apply(lambda x: [t.replace('#','') for t in x])\n",
    "# Remove html chars\n",
    "df['topics'] = df['topics'].apply(lambda x: [t.replace('&gt;','').replace('&lt;','') for t in x])\n",
    "# Remove links\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if \"http\" not in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc84788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all topics that are less than 4 letters\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if len(t) > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051ee7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading words: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     Hostname mismatch, certificate is not valid for\n",
      "[nltk_data]     'raw.githubusercontent.com'. (_ssl.c:1129)>\n"
     ]
    }
   ],
   "source": [
    "# Remove all non english topics\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "def is_english(topic):\n",
    "    a = topic.split(' ')\n",
    "    to_return = []\n",
    "    for x in a:\n",
    "        if x.lower() in english_words:\n",
    "            to_return.append(x)\n",
    "    return ' '.join(to_return)\n",
    "\n",
    "df['topics'] = df['topics'].apply(lambda x: [is_english(t) for t in x])\n",
    "# Remove now empty topics\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if len(t)>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726e8c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def is_noun(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Part of speech tagging\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "    # Check if at least one word is a noun\n",
    "    for word, tag in tagged_words:\n",
    "        if tag == 'NN':\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if is_noun(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33aed76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual clean\n",
    "def is_noun(text):\n",
    "    if len(text.split(\" \")) == 1:\n",
    "        if text.endswith(\"ent\") or text.endswith('er') or text.endswith('est') or text.endswith('ive') or text.endswith('y') or text.endswith(\"ful\") or text.endswith('al') or text.endswith('ung') or text.endswith('th') or text.endswith('ing'):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "df['topics'] = df['topics'].apply(lambda x: [t for t in x if is_noun(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82198d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: Hostname mismatch, certificate is not valid\n",
      "[nltk_data]     for 'raw.githubusercontent.com'. (_ssl.c:1129)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def remove_verbs(df, column):\n",
    "    for i, row in df.iterrows():\n",
    "        strings = row[column]\n",
    "        strings = [string for string in strings if not contains_verb(string)]\n",
    "        # Update the row with the list of strings without verbs\n",
    "        df.at[i, column] = strings\n",
    "    return df\n",
    "\n",
    "def contains_verb(string):\n",
    "    words = nltk.word_tokenize(string)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    # Check if the string contains a verb\n",
    "    return any(tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] for word, tag in tagged_words)\n",
    "\n",
    "df = remove_verbs(df, 'topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70051714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     Hostname mismatch, certificate is not valid for\n",
      "[nltk_data]     'raw.githubusercontent.com'. (_ssl.c:1129)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: Hostname mismatch, certificate is not valid\n",
      "[nltk_data]     for 'raw.githubusercontent.com'. (_ssl.c:1129)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def replace_adj_with_noun(df, column):\n",
    "    for i, row in df.iterrows():\n",
    "        strings = row[column]\n",
    "        new_strings = []\n",
    "        for string in strings:\n",
    "            words = nltk.word_tokenize(string)\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            new_string = []\n",
    "            for word, tag in tagged_words:\n",
    "                # Replace the adjective with its corresponding noun\n",
    "                if tag == 'JJ':\n",
    "                    synsets = wordnet.synsets(word, pos='a')\n",
    "                    if len(synsets) > 0:\n",
    "                        lemmas = synsets[0].lemmas()\n",
    "                        if len(lemmas) > 0:\n",
    "                            p = lemmas[0].pertainyms()\n",
    "                            if len(p) > 0:\n",
    "                                new_word = p[0].name()\n",
    "                                new_string.append(new_word)\n",
    "                                continue\n",
    "                new_string.append(word)\n",
    "            new_strings.append(' '.join(new_string))\n",
    "        # Update the row with the list of strings with replaced adjectives\n",
    "        df.at[i, column] = new_strings\n",
    "    return df\n",
    "\n",
    "df = replace_adj_with_noun(df, 'topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b92573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def uniform_strings(df, column):\n",
    "    # Create a dictionary to store the mapping from words to uniformed strings\n",
    "    mapping = {}\n",
    "    uniformed_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        strings = row[column]\n",
    "        for j, string in enumerate(strings):\n",
    "            # Split the string into words and sort them\n",
    "            words = string.split()\n",
    "            words.sort()\n",
    "            # Join the words back together to create a key for the mapping\n",
    "            key = \" \".join(words)\n",
    "            # If the key is not in the mapping, add it and the original string\n",
    "            if key not in mapping:\n",
    "                mapping[key] = string\n",
    "                uniformed_strings.append(string)\n",
    "            # Replace the string with the uniformed string\n",
    "            strings[j] = mapping[key]\n",
    "        # Update the row with the uniformed strings\n",
    "        df.at[i, column] = strings\n",
    "    return df\n",
    "\n",
    "df = uniform_strings(df, 'topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19975575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "combi = set([t for x in list(df['topics']) for t in x if len(t.split(\" \")) > 1])\n",
    "\n",
    "def check_combi(strings):\n",
    "    new_list = strings\n",
    "    word_combinations = list(combinations(strings, 2))\n",
    "    word_combinations = [t for t in word_combinations if len(t) < 3]\n",
    "    for word in word_combinations:\n",
    "        if ' '.join(word) in combi:\n",
    "            if word[0] in new_list:\n",
    "                new_list.remove(word[0])\n",
    "            if word[1] in new_list:\n",
    "                new_list.remove(word[1])\n",
    "            new_list.append(' '.join(word))\n",
    "    return new_list\n",
    "\n",
    "df['topics'] = df['topics'].apply(check_combi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "507e54c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments: 20752\n",
      "Number of comments after cleaning: 20752\n"
     ]
    }
   ],
   "source": [
    "# Remove all comments without topics\n",
    "print(f'Number of comments: {len(df.id.unique())}')\n",
    "df = df.loc[df['topics'].apply(lambda x: len(x) > 0)]\n",
    "print(f'Number of comments after cleaning: {len(df.id.unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('comments_2022_step2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fe713",
   "metadata": {},
   "source": [
    "# Enrichment with sub-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('comments_2022_step2.csv', header=0)\n",
    "df['topics'] = df['topics'].apply(lambda x: x.replace('[','').replace(']','').replace(\"'\",'').split(', '))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e96aec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove AutoModerator\n",
    "df = df.loc[df['author']!='AutoModerator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "20beb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_keywords = [    \"symbol\", \"art\", \"flag\", \"history\", \"tattoo\", \"mask\", \"tribe\", \"ancestral\",    \"heritage\",    \"totem\",    \"headdress\",    \"regalia\",    \"jewelry\",    \"indigenous\",    \"ceremonial\",    \"ritual\",    \"sacred\",    \"spiritual\",    \"identity\",    \"legacy\"]\n",
    "\n",
    "m_keywords = [    \"hair\", \"haircut\", \"dread\", \"cloth\", \"cornrow\", \"style\", \"pierc\", \"wore\", \"coloniz\", \"wear\", \"misappropriation\",    \"ethnic\",    \"native\",    \"dress\",    \"attire\",    \"outfit\"]\n",
    "\n",
    "l_keywords = [    \"philosoph\", \"english\", \"hola\", \"language\", \"translate\", \"interpreter\", \"name\", \"linguistic\",    \"vernacular\",    \"dialect\",    \"accent\",    \"identity\",    \"heritage\",    \"expression\",    \"borrowing\"]\n",
    "\n",
    "s_keywords = [    \"blackfac\", \"whitewash\", \"song\", \"sing\", \"movie\", \"caricat\", \"equality\", \"character\", \"racis\",\"gatekeep\", \"born\", \"stereotyping\",    \"misrepresentation\",    \"stereotype\",    \"portrayal\",    \"sensitivity\",    \"awareness\",    \"diversity\",    \"respect\",    \"tolerance\"]\n",
    "\n",
    "i_keywords = [    \"ramen\", \"jew\", \"pagan\", \"pizza\", \"preach\", \"gospel\", \"coffee\", \"book\",\"food\", \"christ\", \"cook\", \"recipe\", \"siddur\", \"pray\", \"myth\", \"bow\", \"religion\", \"insensitivity\",    \"religious\",    \"practices\",    \"respect\",    \"tolerance\",    \"identity\",    \"heritage\",    \"expression\"]\n",
    "\n",
    "e_keywords = [   \"compan\", \"anime\", \"commerc\", \"rap\", \"exploit\", \"sexuali\", \"music\", \"exploitation\",    \"commodification\",    \"profit\",    \"commercialization\",    \"tourism\",    \"revenue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "cae4b6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51643/1446546212.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sub-types'] = df['body'].apply(get_subtypes)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>topics</th>\n",
       "      <th>sub-types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hqqvfvb</td>\n",
       "      <td>t1_hqqt3r3</td>\n",
       "      <td>BasicComplexities</td>\n",
       "      <td>VaushV</td>\n",
       "      <td>\\n&amp;gt;. You use French people flippantly, but ...</td>\n",
       "      <td>[racist]</td>\n",
       "      <td>[L, S, M, A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hqqxbu8</td>\n",
       "      <td>t1_hqqhgr2</td>\n",
       "      <td>RagingAardvark</td>\n",
       "      <td>daddit</td>\n",
       "      <td>Thank you! I was actually recently mulling ove...</td>\n",
       "      <td>[culture appropriation, white supremacy, culture]</td>\n",
       "      <td>[L, S, E, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hqr027z</td>\n",
       "      <td>t1_hqqz64g</td>\n",
       "      <td>AdventurousAnxiety78</td>\n",
       "      <td>Afghan</td>\n",
       "      <td>No, the coins are one of many diverse versions...</td>\n",
       "      <td>[traditional dress]</td>\n",
       "      <td>[M, A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hqr0j0w</td>\n",
       "      <td>t3_rt7dak</td>\n",
       "      <td>stjeana</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>Cultural appropriation is vs and doesnt have a...</td>\n",
       "      <td>[culture appropriation, cool, respect, culture]</td>\n",
       "      <td>[I, S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hqr0ska</td>\n",
       "      <td>t3_rt0uqs</td>\n",
       "      <td>kbell2020</td>\n",
       "      <td>Dreadlocks</td>\n",
       "      <td>Hi,\\n\\nWhite mom here, straight hair. My son i...</td>\n",
       "      <td>[culture appropriation]</td>\n",
       "      <td>[M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24417</th>\n",
       "      <td>i6uidyg</td>\n",
       "      <td>t3_ufmjfc</td>\n",
       "      <td>Fafgarth</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>it's quite easy : \\n\\nIf you are white, it's A...</td>\n",
       "      <td>[appreciation]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24418</th>\n",
       "      <td>i6uiqe4</td>\n",
       "      <td>t1_i6sy5fh</td>\n",
       "      <td>datnoob9113</td>\n",
       "      <td>PoliticalCompassMemes</td>\n",
       "      <td>You say this but the view \"cultural appropriat...</td>\n",
       "      <td>[culture appropriation, view, term]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24419</th>\n",
       "      <td>i6uisj4</td>\n",
       "      <td>t1_i6uhf2c</td>\n",
       "      <td>KingCrow27</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>Because those are literally the types of peopl...</td>\n",
       "      <td>[culture appropriation]</td>\n",
       "      <td>[L, I, A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24420</th>\n",
       "      <td>i6uiv27</td>\n",
       "      <td>t3_ufmjfc</td>\n",
       "      <td>Catbunny123</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>Lol how is Korean beauty appropriative? It's j...</td>\n",
       "      <td>[cool, fashion, fashion designer, fashion indu...</td>\n",
       "      <td>[S, E, M, I, A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24421</th>\n",
       "      <td>i6ujsxr</td>\n",
       "      <td>t3_ufggrn</td>\n",
       "      <td>Linguatron</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>I hate to be this guy, but wasn't this *exact*...</td>\n",
       "      <td>[culture appropriation]</td>\n",
       "      <td>[S]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20684 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id   parent_id                author              subreddit  \\\n",
       "0      hqqvfvb  t1_hqqt3r3     BasicComplexities                 VaushV   \n",
       "1      hqqxbu8  t1_hqqhgr2        RagingAardvark                 daddit   \n",
       "3      hqr027z  t1_hqqz64g  AdventurousAnxiety78                 Afghan   \n",
       "4      hqr0j0w   t3_rt7dak               stjeana         TooAfraidToAsk   \n",
       "5      hqr0ska   t3_rt0uqs             kbell2020             Dreadlocks   \n",
       "...        ...         ...                   ...                    ...   \n",
       "24417  i6uidyg   t3_ufmjfc              Fafgarth         TooAfraidToAsk   \n",
       "24418  i6uiqe4  t1_i6sy5fh           datnoob9113  PoliticalCompassMemes   \n",
       "24419  i6uisj4  t1_i6uhf2c            KingCrow27         TooAfraidToAsk   \n",
       "24420  i6uiv27   t3_ufmjfc           Catbunny123         TooAfraidToAsk   \n",
       "24421  i6ujsxr   t3_ufggrn            Linguatron         TooAfraidToAsk   \n",
       "\n",
       "                                                    body  \\\n",
       "0      \\n&gt;. You use French people flippantly, but ...   \n",
       "1      Thank you! I was actually recently mulling ove...   \n",
       "3      No, the coins are one of many diverse versions...   \n",
       "4      Cultural appropriation is vs and doesnt have a...   \n",
       "5      Hi,\\n\\nWhite mom here, straight hair. My son i...   \n",
       "...                                                  ...   \n",
       "24417  it's quite easy : \\n\\nIf you are white, it's A...   \n",
       "24418  You say this but the view \"cultural appropriat...   \n",
       "24419  Because those are literally the types of peopl...   \n",
       "24420  Lol how is Korean beauty appropriative? It's j...   \n",
       "24421  I hate to be this guy, but wasn't this *exact*...   \n",
       "\n",
       "                                                  topics        sub-types  \n",
       "0                                               [racist]     [L, S, M, A]  \n",
       "1      [culture appropriation, white supremacy, culture]     [L, S, E, M]  \n",
       "3                                    [traditional dress]           [M, A]  \n",
       "4        [culture appropriation, cool, respect, culture]           [I, S]  \n",
       "5                                [culture appropriation]              [M]  \n",
       "...                                                  ...              ...  \n",
       "24417                                     [appreciation]               []  \n",
       "24418                [culture appropriation, view, term]               []  \n",
       "24419                            [culture appropriation]        [L, I, A]  \n",
       "24420  [cool, fashion, fashion designer, fashion indu...  [S, E, M, I, A]  \n",
       "24421                            [culture appropriation]              [S]  \n",
       "\n",
       "[20684 rows x 7 columns]"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_subtypes(comment):\n",
    "    sub_types = []\n",
    "    for word in a_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('A')\n",
    "    for word in m_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('M')\n",
    "    for word in l_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('L')\n",
    "    for word in s_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('S')\n",
    "    for word in i_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('I')\n",
    "    for word in e_keywords:\n",
    "        if word in comment.lower():\n",
    "            sub_types.append('E')\n",
    "    return list(set(sub_types))\n",
    "\n",
    "df['sub-types'] = df['body'].apply(get_subtypes)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "fc9987f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>topics</th>\n",
       "      <th>sub-types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hqr659v</td>\n",
       "      <td>t3_rt8733</td>\n",
       "      <td>JustMeHere8431</td>\n",
       "      <td>ireland</td>\n",
       "      <td>Cultural appropriation, we need to cancel him 😂😂</td>\n",
       "      <td>[culture appropriation, rework]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hqrk3bg</td>\n",
       "      <td>t1_hqrjvzf</td>\n",
       "      <td>OkScheme625</td>\n",
       "      <td>Genshin_Impact</td>\n",
       "      <td>what are you even talking about the reason nik...</td>\n",
       "      <td>[culture]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hqrnwut</td>\n",
       "      <td>t1_hqom3d4</td>\n",
       "      <td>e-s-p</td>\n",
       "      <td>AskMen</td>\n",
       "      <td>The internet has given us a disconnect from th...</td>\n",
       "      <td>[information, culture appropriation]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hqryx9u</td>\n",
       "      <td>t3_rtcczg</td>\n",
       "      <td>Jules8432</td>\n",
       "      <td>toronto</td>\n",
       "      <td>Fireworks are cultural appropriation from Chin...</td>\n",
       "      <td>[china]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hqs5d9x</td>\n",
       "      <td>t3_rtd6hj</td>\n",
       "      <td>Competitive-Comb4574</td>\n",
       "      <td>TrueOffMyChest</td>\n",
       "      <td>'cultural appropriation' is a ridiculous concept.</td>\n",
       "      <td>[culture appropriation]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24404</th>\n",
       "      <td>i6ufir2</td>\n",
       "      <td>t3_ufggrn</td>\n",
       "      <td>KungThulhu</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>cultural appropriation is a twitter thing and ...</td>\n",
       "      <td>[culture appropriation]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24405</th>\n",
       "      <td>i6ufl8n</td>\n",
       "      <td>t3_ufggrn</td>\n",
       "      <td>TooBusySaltMining</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>It's a small, very vocal and very stupid minor...</td>\n",
       "      <td>[style, music, culture fashion, culture fashion]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24408</th>\n",
       "      <td>i6ugbqy</td>\n",
       "      <td>t3_ufggrn</td>\n",
       "      <td>anonym_ami</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>America wouldn’t be the melting pot that it is...</td>\n",
       "      <td>[culture appropriation, immigration, immigration]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24417</th>\n",
       "      <td>i6uidyg</td>\n",
       "      <td>t3_ufmjfc</td>\n",
       "      <td>Fafgarth</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>it's quite easy : \\n\\nIf you are white, it's A...</td>\n",
       "      <td>[appreciation]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24418</th>\n",
       "      <td>i6uiqe4</td>\n",
       "      <td>t1_i6sy5fh</td>\n",
       "      <td>datnoob9113</td>\n",
       "      <td>PoliticalCompassMemes</td>\n",
       "      <td>You say this but the view \"cultural appropriat...</td>\n",
       "      <td>[culture appropriation, view, term]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6457 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id   parent_id                author              subreddit  \\\n",
       "8      hqr659v   t3_rt8733        JustMeHere8431                ireland   \n",
       "14     hqrk3bg  t1_hqrjvzf           OkScheme625         Genshin_Impact   \n",
       "17     hqrnwut  t1_hqom3d4                 e-s-p                 AskMen   \n",
       "24     hqryx9u   t3_rtcczg             Jules8432                toronto   \n",
       "27     hqs5d9x   t3_rtd6hj  Competitive-Comb4574         TrueOffMyChest   \n",
       "...        ...         ...                   ...                    ...   \n",
       "24404  i6ufir2   t3_ufggrn            KungThulhu         TooAfraidToAsk   \n",
       "24405  i6ufl8n   t3_ufggrn     TooBusySaltMining         TooAfraidToAsk   \n",
       "24408  i6ugbqy   t3_ufggrn            anonym_ami         TooAfraidToAsk   \n",
       "24417  i6uidyg   t3_ufmjfc              Fafgarth         TooAfraidToAsk   \n",
       "24418  i6uiqe4  t1_i6sy5fh           datnoob9113  PoliticalCompassMemes   \n",
       "\n",
       "                                                    body  \\\n",
       "8       Cultural appropriation, we need to cancel him 😂😂   \n",
       "14     what are you even talking about the reason nik...   \n",
       "17     The internet has given us a disconnect from th...   \n",
       "24     Fireworks are cultural appropriation from Chin...   \n",
       "27     'cultural appropriation' is a ridiculous concept.   \n",
       "...                                                  ...   \n",
       "24404  cultural appropriation is a twitter thing and ...   \n",
       "24405  It's a small, very vocal and very stupid minor...   \n",
       "24408  America wouldn’t be the melting pot that it is...   \n",
       "24417  it's quite easy : \\n\\nIf you are white, it's A...   \n",
       "24418  You say this but the view \"cultural appropriat...   \n",
       "\n",
       "                                                  topics sub-types  \n",
       "8                        [culture appropriation, rework]        []  \n",
       "14                                             [culture]        []  \n",
       "17                  [information, culture appropriation]        []  \n",
       "24                                               [china]        []  \n",
       "27                               [culture appropriation]        []  \n",
       "...                                                  ...       ...  \n",
       "24404                            [culture appropriation]        []  \n",
       "24405   [style, music, culture fashion, culture fashion]        []  \n",
       "24408  [culture appropriation, immigration, immigration]        []  \n",
       "24417                                     [appreciation]        []  \n",
       "24418                [culture appropriation, view, term]        []  \n",
       "\n",
       "[6457 rows x 7 columns]"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['sub-types'].map(len)==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "b4df4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['subreddit']!='[deleted]']\n",
    "df = df.loc[df['author']!='[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "3fde8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('comments_2022_step3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
