{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a86101",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceeb115",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLN = pickle.load(open('../Multilayer Network/MLN.pickle', 'rb'))\n",
    "content_layer = pickle.load(open('../Multilayer Network/content_layer.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b98fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PageRank\n",
    "pagerank = nx.pagerank(content_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Clustering Coefficient (Parallelized because it is slow)\n",
    "import networkx as nx\n",
    "import concurrent.futures\n",
    "\n",
    "def compute_clustering(node):\n",
    "    triangles = sum(1 for v in nx.all_neighbors(content_layer, node) if v in content_layer[node])\n",
    "    degree = content_layer.degree(node)\n",
    "    if degree < 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2.0 * triangles / (degree * (degree - 1))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    clustering = {n: executor.submit(compute_clustering, n) for n in content_layer.nodes()}\n",
    "    clustering = {n: f.result() if content_layer.degree(n) >= 2 else 0 for n, f in clustering.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3625ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centralities = nx.degree_centrality(content_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c44a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value normalization\n",
    "def normalize_values(dictionary):\n",
    "    min_value = min(dictionary.values())\n",
    "    max_value = max(dictionary.values())\n",
    "    for user in dictionary.keys():\n",
    "        try:\n",
    "            dictionary[user] = (dictionary[user] - min_value) / (max_value - min_value)\n",
    "        except:\n",
    "            dictionary[user] = 1\n",
    "    return dictionary\n",
    "\n",
    "pagerank = normalize_values(pagerank)\n",
    "degree_centralities = normalize_values(degree_centralities)\n",
    "clustering_coefficient = normalize_values(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7cedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Extract the values from the dictionaries\n",
    "pagerank_values = list(pagerank.values())\n",
    "closeness_centrality_values = list(degree_centralities.values())\n",
    "clustering_coefficient_values = list(clustering_coefficient.values())\n",
    "\n",
    "# Plot the distributions in 3 different plots in a same figure\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax1.hist(pagerank_values, bins=100, color='blue')\n",
    "ax1.set_title('Pagerank Distribution')\n",
    "ax2.hist(closeness_centrality_values, bins=100, color='red')\n",
    "ax2.set_title('Degree Centrality Distribution')\n",
    "ax3.hist(clustering_coefficient_values, bins=100, color='green')\n",
    "ax3.set_title('Clustering Coefficient Distribution')\n",
    "plt.savefig('PC_CC.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f94fa",
   "metadata": {},
   "source": [
    "# Get key contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biggest peak in pagerank\n",
    "biggest_peak_pagerank = {}\n",
    "for pg in pagerank:\n",
    "    if pagerank[pg] >= 0.5 and pagerank[pg] <= 0.6:\n",
    "        biggest_peak_pagerank[pg] = pagerank[pg]\n",
    "print(len(biggest_peak_pagerank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biggest peak in degree centrality\n",
    "biggest_peak_degree = {}\n",
    "for dg in degree_centralities:\n",
    "    if degree_centralities[dg] >= 0.78 and degree_centralities[dg] <= 0.83:\n",
    "        biggest_peak_degree[dg] = degree_centralities[dg]\n",
    "print(len(biggest_peak_degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = set(biggest_peak_pagerank.keys()).intersection(set(biggest_peak_degree.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b642db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topics percentage in edges for key contents\n",
    "labels = nx.get_edge_attributes(content_layer, 'label')\n",
    "labels2 = {label: topics for label, topics in labels.items() if intersection & set(label)}\n",
    "topics = {}\n",
    "for label_topics in labels2.values():\n",
    "    for topic in label_topics:\n",
    "        try:\n",
    "            topics[topic] += 1\n",
    "        except:\n",
    "            topics[topic] = 1\n",
    "\n",
    "for t in topics:\n",
    "    topics[t] = (topics[t]/len(labels2))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301a51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot topic counts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Order topic counts by number of occurrences\n",
    "sorted_topic_counts = dict(sorted(topics.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Plot the top 50 topics\n",
    "labels = list(sorted_topic_counts.keys())[:10]\n",
    "values = list(sorted_topic_counts.values())[:10]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(labels, values)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Percentage of presence in the edges')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('topics_key_contents.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1bca8",
   "metadata": {},
   "source": [
    "# Projection of key contents on User layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d79526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-graph of key contents\n",
    "keytopics_net = content_layer.subgraph(list(intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "multi_layer_edges = [(u,v,data) for u,v,data in MLN.edges(data=True) if v in keytopics_net.nodes and data['layer']=='multi_u_c']\n",
    "t_user_layer = nx.Graph()\n",
    "for t in keytopics_net.nodes:\n",
    "    t_user_layer.add_node(t)\n",
    "user_to_t_map = {}\n",
    "for edge in multi_layer_edges:\n",
    "    if edge[0] not in user_to_t_map:\n",
    "        user_to_t_map[edge[0]] = []\n",
    "    user_to_t_map[edge[0]].append(edge[1])\n",
    "user_to_t_map_filtered = {}\n",
    "for c in user_to_t_map:\n",
    "    if len(user_to_t_map[c]) > 1:\n",
    "        user_to_t_map_filtered[c] = user_to_t_map[c]\n",
    "comments = pd.read_csv('../Dataset/comments_2022.csv', header=0)\n",
    "result = {}\n",
    "for user_node, content_nodes in user_to_t_map_filtered.items():\n",
    "    subs = []\n",
    "    for c in content_nodes:\n",
    "        subs.append(comments[comments['id'] == c]['subreddit'].item())\n",
    "    subs = list(set(subs))\n",
    "    for i, c1 in enumerate(content_nodes):\n",
    "        for c2 in content_nodes[i+1:]:\n",
    "            try:\n",
    "                result[(c1,c2)].extend(subs)\n",
    "                result[(c1,c2)] = list(set(result[(c1,c2)]))\n",
    "            except:\n",
    "                result[(c1,c2)] = subs\n",
    "for res in result.keys():\n",
    "    t_user_layer.add_edge(res[0], res[1], label=result[res])\n",
    "pickle.dump(t_user_layer, open('t_user_layer.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a190735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "print(len(t_user_layer.nodes))\n",
    "print(len(t_user_layer.edges))\n",
    "print(nx.number_of_isolates(t_user_layer))\n",
    "print(nx.density(t_user_layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5cfd5",
   "metadata": {},
   "source": [
    "# Key contents and key users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e246c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key users in the projection\n",
    "key_users = pickle.load(open('key_users.pickle', 'rb'))\n",
    "len(set(user_to_t_map.keys()).intersection(set(key_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key users in AmItheAsshole\n",
    "key_users_comments = comments.loc[comments['author'].isin(key_users)]\n",
    "len(key_users_comments.loc[(key_users_comments['subreddit']=='AmItheAsshole') & ()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671ba7d",
   "metadata": {},
   "source": [
    "# Subreddit analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de86c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subreddit counts distribution\n",
    "community_counts = {}\n",
    "for key, value  in result.items():\n",
    "    for sub in value:\n",
    "        try:\n",
    "            community_counts[sub] += 1\n",
    "        except:\n",
    "            community_counts[sub] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e196bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot subreddit distribution\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Order topic counts by number of occurrences\n",
    "sorted_community_counts = dict(sorted(community_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Plot the top 50 topics\n",
    "plt.figure(figsize=(10,5))\n",
    "labels = list(sorted_community_counts.keys())[:50]\n",
    "values = list(sorted_community_counts.values())[:50]\n",
    "plt.bar(labels, values)\n",
    "plt.xlabel('Subreddit')\n",
    "plt.ylabel('Number of key contents')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('key_contents_subs.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
