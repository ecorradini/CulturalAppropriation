{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6178d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331652b4",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLN = pickle.load(open('../Multilayer Network/MLN.pickle', 'rb'))\n",
    "user_layer = pickle.load(open('../Multilayer Network/user_layer.pickle', 'rb'))\n",
    "content_layer = pickle.load(open('../Multilayer Network/content_layer.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b05b8e",
   "metadata": {},
   "source": [
    "# Metrics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8775bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def compute_centralities(network, centrality_func):\n",
    "    centralities = nx.degree_centrality(network) if centrality_func == 'degree' else \\\n",
    "                  nx.betweenness_centrality(network) if centrality_func == 'betweenness' else \\\n",
    "                  nx.eigenvector_centrality(network) if centrality_func == 'eigenvector' else {}\n",
    "    return centralities\n",
    "\n",
    "def parallel_compute_centralities(network, centrality_funcs):\n",
    "    pool = mp.Pool(processes=mp.cpu_count())\n",
    "    results = [pool.apply_async(compute_centralities, args=(network, centrality_func)) for centrality_func in centrality_funcs]\n",
    "    centralities = [r.get() for r in results]\n",
    "    return centralities\n",
    "\n",
    "centrality_funcs = ['degree', 'betweenness', 'eigenvector']\n",
    "centralities = parallel_compute_centralities(user_layer, centrality_funcs)\n",
    "degree_centralities, betweenness_centralities, eigenvector_centralities = centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(dictionary):\n",
    "    min_value = min(dictionary.values())\n",
    "    max_value = max(dictionary.values())\n",
    "    for user in dictionary.keys():\n",
    "        dictionary[user] = (dictionary[user] - min_value) / (max_value - min_value)\n",
    "    return dictionary\n",
    "\n",
    "degree_centralities = normalize_values(degree_centralities)\n",
    "betweenness_centralities = normalize_values(betweenness_centralities)\n",
    "eigenvector_centralities = normalize_values(eigenvector_centralities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2006a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Extract the values from the dictionaries\n",
    "degree_values = list(degree_centralities.values())\n",
    "betweenness_values = list(betweenness_centralities.values())\n",
    "eigenvector_values = list(eigenvector_centralities.values())\n",
    "\n",
    "# Plot the distributions in 3 different plots in a same figure\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax1.hist(degree_values, bins=100, color='blue')\n",
    "ax1.set_ylim(0,17500)\n",
    "ax1.set_title('Degree Centrality Distribution')\n",
    "ax2.hist(betweenness_values, bins=100, color='red')\n",
    "ax2.set_ylim(0,17500)\n",
    "ax2.set_title('Betweenness Centrality Distribution')\n",
    "ax3.hist(eigenvector_values, bins=100, color='green')\n",
    "ax3.set_ylim(0,17500)\n",
    "ax3.set_title('Eigenvector Centrality Distribution')\n",
    "plt.savefig('centralities.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cdbdc3",
   "metadata": {},
   "source": [
    "# TOPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def topsis(degree_centralities, betweenness_centralities, eigenvector_centralities, weights, impact):\n",
    "    n = len(degree_centralities)\n",
    "    decision_matrix = np.zeros((n, 3))\n",
    "    decision_matrix[:, 0] = list(degree_centralities.values())\n",
    "    decision_matrix[:, 1] = list(betweenness_centralities.values())\n",
    "    decision_matrix[:, 2] = list(eigenvector_centralities.values())\n",
    "    weighted_matrix = decision_matrix * weights\n",
    "    normalized_matrix = weighted_matrix / np.linalg.norm(weighted_matrix, axis=0)\n",
    "    ideal_positive = np.max(normalized_matrix, axis=0) * impact\n",
    "    ideal_negative = np.min(normalized_matrix, axis=0) * impact\n",
    "    distances_positive = np.linalg.norm(ideal_positive - normalized_matrix, axis=1)\n",
    "    distances_negative = np.linalg.norm(normalized_matrix - ideal_negative, axis=1)\n",
    "    relative_closeness = distances_negative / (distances_negative + distances_positive)\n",
    "    key_users = [user for user, value in zip(degree_centralities.keys(), relative_closeness) if value >= np.mean(relative_closeness)]\n",
    "    return key_users\n",
    "\n",
    "weights = np.array([0.33, 0.33, 0.34]) # assign equal weights to each criterion\n",
    "impact = np.array([1, 1, 1]) # assign positive impact to each criterion\n",
    "key_users = topsis(degree_centralities, betweenness_centralities, eigenvector_centralities, weights, impact)\n",
    "pickle.dump(key_users, open('key_users.pickle', 'wb'))\n",
    "print(len(key_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8d418",
   "metadata": {},
   "source": [
    "# Projection of key users on Content layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c905046",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyusers_net = user_layer.subgraph(key_users)\n",
    "len(keyusers_net.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea8eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.density(keyusers_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content_layer = nx.Graph()\n",
    "for u in key_users:\n",
    "    user_content_layer.add_node(u)\n",
    "multi_layer_edges = [(u,v,data) for u,v,data in MLN.edges(data=True) if data['layer']=='multi_u_c']\n",
    "multi_layer_edges = [edge for edge in multi_layer_edges if edge[0] in key_users or edge[1] in key_users]\n",
    "content_edges = [(c1,c2,data) for c1,c2,data in MLN.edges(data=True) if data['layer']=='content']\n",
    "content_to_user_map = {}\n",
    "for edge in multi_layer_edges:\n",
    "    if edge[1] not in content_to_user_map:\n",
    "        content_to_user_map[edge[1]] = []\n",
    "    content_to_user_map[edge[1]].append(edge[0])\n",
    "content_to_user_map_filtered = {}\n",
    "for c in content_to_user_map:\n",
    "    if len(content_to_user_map[c]) > 1:\n",
    "        content_to_user_map_filtered[c] = content_to_user_map[c]\n",
    "content_edges = [(c1,c2,data) for c1,c2,data in content_edges if c1 in content_to_user_map_filtered.keys() or c2 in content_to_user_map_filtered.keys()]\n",
    "result = {}\n",
    "for content_node, user_nodes in content_to_user_map_filtered.items():\n",
    "    for c1,c2,data in content_edges:\n",
    "        topics = []\n",
    "        if (c1 == content_node or c2 == content_node) and data['layer'] == 'content':\n",
    "            topics.extend(data['label'])\n",
    "        topics = list(set(topics))\n",
    "        for i, u1 in enumerate(user_nodes):\n",
    "            for u2 in user_nodes[i+1:]:\n",
    "                try:\n",
    "                    result[(u1,u2)].extend(topics)\n",
    "                    result[(u1,u2)] = list(set(result[(u1,u2)]))\n",
    "                except:\n",
    "                    result[(u1,u2)] = topics\n",
    "for res in result.keys():\n",
    "    user_content_layer.add_edge(res[0], res[1], label=result[res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e87fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "print(len(user_content_layer.nodes))\n",
    "print(len(user_content_layer.edges))\n",
    "print(nx.density(user_content_layer))\n",
    "print(nx.number_of_isolates(user_content_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot topics for key users\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "all_topics = []\n",
    "for topic_list in result.values():\n",
    "    all_topics.extend(topic_list)\n",
    "topic_counts = dict()\n",
    "for topic in all_topics:\n",
    "    if topic in topic_counts:\n",
    "        topic_counts[topic] += 1\n",
    "    else:\n",
    "        topic_counts[topic] = 1\n",
    "\n",
    "# Order topic counts by number of occurrences\n",
    "sorted_topic_counts = dict(sorted(topic_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Plot the top 50 topics\n",
    "labels = list(sorted_topic_counts.keys())[:50]\n",
    "values = list(sorted_topic_counts.values())[:50]\n",
    "plt.bar(labels, values)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of Occurrences')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('topics_key.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
